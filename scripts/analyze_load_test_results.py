#!/usr/bin/env python
"""
Load Test Results Analyzer for Credit Cashflow Engine

This script analyzes Locust load test results, provides performance insights,
and can be used to validate performance SLAs in CI/CD pipelines.

Usage:
    python analyze_load_test_results.py --csv results --threshold-rps 50 --threshold-error-rate 0.01
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional


class LoadTestAnalyzer:
    """Analyzes Locust load test results and generates reports."""

    def __init__(self, 
                 csv_prefix: str, 
                 threshold_rps: float = None,
                 threshold_p95: float = None,
                 threshold_error_rate: float = None):
        """
        Initialize the analyzer with result files and thresholds.
        
        Args:
            csv_prefix: The prefix of the CSV files generated by Locust
            threshold_rps: Minimum acceptable requests per second
            threshold_p95: Maximum acceptable p95 response time in ms
            threshold_error_rate: Maximum acceptable error rate (0-1)
        """
        self.csv_prefix = csv_prefix
        self.threshold_rps = threshold_rps
        self.threshold_p95 = threshold_p95
        self.threshold_error_rate = threshold_error_rate
        
        # Result storage
        self.stats_df = None
        self.history_df = None
        self.failures_df = None
        self.exceptions_df = None
        
        # Performance summary
        self.summary = {}
        
        # Load the CSV files
        self._load_csv_files()
    
    def _load_csv_files(self) -> None:
        """Load the CSV files produced by Locust."""
        stats_path = f"{self.csv_prefix}_stats.csv"
        history_path = f"{self.csv_prefix}_stats_history.csv"
        failures_path = f"{self.csv_prefix}_failures.csv"
        exceptions_path = f"{self.csv_prefix}_exceptions.csv"
        
        try:
            self.stats_df = pd.read_csv(stats_path)
            self.history_df = pd.read_csv(history_path)
            
            # These files might not exist if there were no failures/exceptions
            if os.path.exists(failures_path):
                self.failures_df = pd.read_csv(failures_path)
            
            if os.path.exists(exceptions_path):
                self.exceptions_df = pd.read_csv(exceptions_path)
                
            print(f"Successfully loaded data from {self.csv_prefix} CSV files")
        except Exception as e:
            print(f"Error loading CSV files: {e}")
            sys.exit(1)
    
    def analyze(self) -> Dict[str, Any]:
        """
        Analyze the load test results and return a summary.
        
        Returns:
            A dictionary containing the analysis results
        """
        if self.stats_df is None or self.history_df is None:
            print("No data to analyze")
            return {}
        
        # Calculate overall statistics
        total_requests = self.stats_df['Total Request Count'].sum()
        total_failures = self.stats_df['Failure Count'].sum()
        error_rate = total_failures / total_requests if total_requests > 0 else 0
        
        # Calculate average RPS across the test
        test_duration = self.history_df['Timestamp'].max() - self.history_df['Timestamp'].min()
        avg_rps = total_requests / (test_duration / 1000) if test_duration > 0 else 0
        
        # Get peak RPS
        peak_rps = self.history_df['Total RPS'].max()
        
        # Calculate response time statistics for all requests
        overall_response_times = {
            'avg': self.stats_df['Average Response Time'].mean(),
            'min': self.stats_df['Min Response Time'].min(),
            'max': self.stats_df['Max Response Time'].max(),
            'p50': self.stats_df['50%'].mean(),
            'p95': self.stats_df['95%'].mean(),
            'p99': self.stats_df['99%'].mean()
        }
        
        # Find the slowest endpoint
        slowest_idx = self.stats_df['Average Response Time'].idxmax()
        slowest_endpoint = {
            'name': self.stats_df.loc[slowest_idx, 'Name'],
            'avg_response_time': self.stats_df.loc[slowest_idx, 'Average Response Time'],
            'p95_response_time': self.stats_df.loc[slowest_idx, '95%']
        }
        
        # Find the endpoint with highest error rate
        error_rates = self.stats_df['Failure Count'] / self.stats_df['Total Request Count']
        error_rates = error_rates.fillna(0)
        highest_error_idx = error_rates.idxmax()
        highest_error_endpoint = {
            'name': self.stats_df.loc[highest_error_idx, 'Name'],
            'error_rate': error_rates[highest_error_idx],
            'failures': self.stats_df.loc[highest_error_idx, 'Failure Count']
        }
        
        # Check if any thresholds are violated
        threshold_violations = []
        if self.threshold_rps is not None and avg_rps < self.threshold_rps:
            threshold_violations.append(f"RPS below threshold: {avg_rps:.2f} < {self.threshold_rps}")
        
        if self.threshold_p95 is not None and overall_response_times['p95'] > self.threshold_p95:
            threshold_violations.append(
                f"P95 response time above threshold: {overall_response_times['p95']:.2f}ms > {self.threshold_p95}ms"
            )
        
        if self.threshold_error_rate is not None and error_rate > self.threshold_error_rate:
            threshold_violations.append(
                f"Error rate above threshold: {error_rate:.4f} > {self.threshold_error_rate}"
            )
        
        # Compile summary
        self.summary = {
            'total_requests': total_requests,
            'total_failures': total_failures,
            'error_rate': error_rate,
            'avg_rps': avg_rps,
            'peak_rps': peak_rps,
            'test_duration_seconds': test_duration / 1000,
            'response_times': overall_response_times,
            'slowest_endpoint': slowest_endpoint,
            'highest_error_endpoint': highest_error_endpoint,
            'threshold_violations': threshold_violations,
            'passed': len(threshold_violations) == 0
        }
        
        return self.summary
    
    def print_report(self) -> None:
        """Print a formatted report of the load test results."""
        if not self.summary:
            self.analyze()
        
        s = self.summary
        
        print("\n" + "=" * 80)
        print(f"LOAD TEST RESULTS ANALYSIS")
        print("=" * 80)
        
        print(f"\nTest Duration: {s['test_duration_seconds']:.2f} seconds")
        print(f"Total Requests: {s['total_requests']}")
        print(f"Failed Requests: {s['total_failures']}")
        print(f"Error Rate: {s['error_rate']*100:.2f}%")
        
        print("\nPerformance:")
        print(f"  Average RPS: {s['avg_rps']:.2f}")
        print(f"  Peak RPS: {s['peak_rps']:.2f}")
        print(f"  Avg Response Time: {s['response_times']['avg']:.2f}ms")
        print(f"  P95 Response Time: {s['response_times']['p95']:.2f}ms")
        print(f"  P99 Response Time: {s['response_times']['p99']:.2f}ms")
        
        print("\nEndpoint Analysis:")
        print(f"  Slowest Endpoint: {s['slowest_endpoint']['name']}")
        print(f"    Avg Response Time: {s['slowest_endpoint']['avg_response_time']:.2f}ms")
        print(f"    P95 Response Time: {s['slowest_endpoint']['p95_response_time']:.2f}ms")
        
        print(f"\n  Highest Error Rate: {s['highest_error_endpoint']['name']}")
        print(f"    Error Rate: {s['highest_error_endpoint']['error_rate']*100:.2f}%")
        print(f"    Total Failures: {s['highest_error_endpoint']['failures']}")
        
        if self.failures_df is not None and len(self.failures_df) > 0:
            print("\nTop Failure Reasons:")
            for i, (_, row) in enumerate(self.failures_df.iterrows(), 1):
                if i > 5:  # Show top 5 failures
                    break
                print(f"  {i}. {row['Method']} {row['Name']}: {row['Error']}")
                print(f"     Occurrences: {row['Occurrences']}")
        
        if len(s['threshold_violations']) > 0:
            print("\nTHRESHOLD VIOLATIONS:")
            for violation in s['threshold_violations']:
                print(f"  - {violation}")
            print("\nLoad Test Result: FAILED")
        else:
            print("\nLoad Test Result: PASSED")
            
            if self.threshold_rps:
                print(f"  - RPS threshold met: {s['avg_rps']:.2f} >= {self.threshold_rps}")
            if self.threshold_p95:
                print(f"  - P95 threshold met: {s['response_times']['p95']:.2f}ms <= {self.threshold_p95}ms")
            if self.threshold_error_rate:
                print(f"  - Error rate threshold met: {s['error_rate']*100:.2f}% <= {self.threshold_error_rate*100:.2f}%")
        
        print("=" * 80 + "\n")
    
    def generate_charts(self, output_dir: str = '.') -> None:
        """
        Generate charts for visualizing the load test results.
        
        Args:
            output_dir: Directory to save the generated charts
        """
        if self.history_df is None:
            print("No history data available for charts")
            return
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Convert timestamp to seconds from start
        self.history_df['Seconds'] = (self.history_df['Timestamp'] - self.history_df['Timestamp'].min()) / 1000
        
        # Chart 1: RPS over time
        plt.figure(figsize=(12, 6))
        plt.plot(self.history_df['Seconds'], self.history_df['Total RPS'], label='Total RPS')
        plt.xlabel('Time (seconds)')
        plt.ylabel('Requests per Second')
        plt.title('Requests per Second Over Time')
        plt.grid(True)
        plt.legend()
        plt.tight_layout()
        plt.savefig(output_path / 'rps_over_time.png')
        
        # Chart 2: Response times over time
        plt.figure(figsize=(12, 6))
        plt.plot(self.history_df['Seconds'], self.history_df['Average Response Time'], label='Average')
        plt.plot(self.history_df['Seconds'], self.history_df['95%'], label='95th Percentile')
        plt.xlabel('Time (seconds)')
        plt.ylabel('Response Time (ms)')
        plt.title('Response Times Over Time')
        plt.grid(True)
        plt.legend()
        plt.tight_layout()
        plt.savefig(output_path / 'response_times.png')
        
        # Chart 3: Error rate over time
        if 'Total Failure Count' in self.history_df.columns:
            error_rate = self.history_df['Total Failure Count'] / self.history_df['Total Request Count']
            error_rate = error_rate.fillna(0) * 100  # Convert to percentage
            
            plt.figure(figsize=(12, 6))
            plt.plot(self.history_df['Seconds'], error_rate, 'r-', label='Error Rate')
            plt.xlabel('Time (seconds)')
            plt.ylabel('Error Rate (%)')
            plt.title('Error Rate Over Time')
            plt.grid(True)
            plt.legend()
            plt.tight_layout()
            plt.savefig(output_path / 'error_rate.png')
        
        # Chart 4: Response time distribution by endpoint
        if len(self.stats_df) > 0:
            endpoints = self.stats_df['Name'].tolist()
            avg_times = self.stats_df['Average Response Time'].tolist()
            p95_times = self.stats_df['95%'].tolist()
            
            # Limit to top 10 endpoints by response time
            if len(endpoints) > 10:
                indices = np.argsort(p95_times)[-10:]
                endpoints = [endpoints[i] for i in indices]
                avg_times = [avg_times[i] for i in indices]
                p95_times = [p95_times[i] for i in indices]
            
            plt.figure(figsize=(12, 8))
            x = np.arange(len(endpoints))
            width = 0.35
            
            plt.bar(x - width/2, avg_times, width, label='Average')
            plt.bar(x + width/2, p95_times, width, label='P95')
            
            plt.xlabel('Endpoint')
            plt.ylabel('Response Time (ms)')
            plt.title('Response Time by Endpoint')
            plt.xticks(x, [e[:30] + '...' if len(e) > 30 else e for e in endpoints], rotation=45, ha='right')
            plt.legend()
            plt.tight_layout()
            plt.savefig(output_path / 'endpoint_response_times.png')
        
        print(f"Charts saved to {output_path}")
    
    def save_results_json(self, output_file: str) -> None:
        """
        Save the analysis results to a JSON file.
        
        Args:
            output_file: Path to save the JSON file
        """
        import json
        
        if not self.summary:
            self.analyze()
        
        # Convert numpy values to Python native types for JSON serialization
        def convert_numpy(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return obj
        
        summary_dict = {k: convert_numpy(v) for k, v in self.summary.items()}
        
        with open(output_file, 'w') as f:
            json.dump(summary_dict, f, indent=2)
        
        print(f"Results saved to {output_file}")


def main():
    """Main function to run the load test analysis."""
    parser = argparse.ArgumentParser(description="Analyze Locust load test results")
    parser.add_argument("--csv", required=True, help="Prefix of the Locust CSV files")
    parser.add_argument("--threshold-rps", type=float, help="Minimum acceptable requests per second")
    parser.add_argument("--threshold-p95", type=float, help="Maximum acceptable p95 response time in ms")
    parser.add_argument("--threshold-error-rate", type=float, help="Maximum acceptable error rate (0-1)")
    parser.add_argument("--output-dir", default=".", help="Directory to save charts")
    parser.add_argument("--save-json", help="Save results to JSON file")
    parser.add_argument("--exit-on-fail", action="store_true", help="Exit with non-zero code on failure")
    
    args = parser.parse_args()
    
    analyzer = LoadTestAnalyzer(
        csv_prefix=args.csv,
        threshold_rps=args.threshold_rps,
        threshold_p95=args.threshold_p95,
        threshold_error_rate=args.threshold_error_rate
    )
    
    results = analyzer.analyze()
    analyzer.print_report()
    
    if args.output_dir:
        analyzer.generate_charts(args.output_dir)
    
    if args.save_json:
        analyzer.save_results_json(args.save_json)
    
    if args.exit_on_fail and not results.get('passed', True):
        sys.exit(1)
    
    sys.exit(0)


if __name__ == "__main__":
    main()
